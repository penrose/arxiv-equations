At a first glance, to build a neural decoder, we can simply train a convolutional neural net with input-output pairs (syndrome, logical correction).
However, in practice, this does not allow us to get a good enough performance.
A detailed description of some simpler approaches and discussion will be presented in \aref{appendix:simpler_approach}.
Those failures eventually motivate us to design and train the neural decoder in the following way.

\subsection{Design of the network}
\label{subsec:design_network}
The network follows the same structure as the renormalization decoder.
Most of the network is repetitively applying the renormalization block, which is depicted in \autoref{subfig:rn_block}.
The belief propagation (BP) network, as its name suggested, is intended to approximate the BP process (see \aref{appendix:bp} for an introduction).
More concretely, the first step of the training process is to train the BP network with the data generated by a handcrafted BP algorithm.
This means initially the inputs to the BP network are syndromes and error rates on each edge, and the outputs are supposed to approximate the error rates on coarse-grained edges.
However, later in the training process (i.e. global training mentioned in \autoref{subsec:training}), the BP network can deviate from this initial behavior.
The post-processing has two steps.
The first step is to remove the superficial complexity from the coarse-grained lattice.
Whenever for a coarse-grained edge $e$ has $p_e(1)>0.5$, we apply an $X$ on $e$ and switch $p_e(1) \leftrightarrow p_e(0)$.
If $e$ is on either of the two non-contractible loops $l_{1,2}$, then the desired logical correction will be updated accordingly.
Although this step only changes the representation of the data, and in principle, neural nets can learn to do the same thing, it is a quite costly step for neural nets as it can call the parity function multiple times.
The second step is coarse-graining.
We need to reduce the lattice size by half, and for convenience, this is done by the first layer of every belief propagation network.
We also compute the parity of four $\langle B_p \rangle$ in each unit cell and feed these parities to the next BP network as the effective syndrome of the coarse-grained lattice.

\begin{figure}

	\includegraphics[width=.7\linewidth]{rn_block}
	\caption{Structure of each renormalization block (RN block).}
	\label{subfig:rn_block}
\end{figure}

\begin{figure}
	\includegraphics[width=.4\linewidth]{complete_network}
	\caption{Structure of the entire network, and the training order labeled by the blue circles. After loading the pre-trained belief propagation network into RN block, the first step is to train the dense layers, and the second step is to train all the layers together. We will call the second step global training.}
	\label{subfig:complete_network}

\end{figure}

In more detail, the input to the BP network can be packed in a tensor $I$ with shape $(l,l,3)$, where $l$ is the initial lattice size or the output size of the precedent BP network.
For example, we can set $I(i,j,0)$ to be $\langle B_p \rangle$ on plaquette $(i,j)$, and $I(i,j,1), I(i,j,2)$ to be the error rates corresponding to the top and left qubits of the plaquette.
Each BP network consists of 13 convolution and 3 batch normalization layers.
The definition of convolution layers can be found in \aref{appendix:intro_nn}, and batch normalization is introduced in~\cite{Ioffe2015BatchNormalization}.
The first layer reduces the lattice size $L$ by half.
The reasoning is that the belief propagation is done based on $2\times 2$ unit cells.
The remaining layers keep the lattice size unchanged.
Among them, only four involve communication between unit cells, i.e. the kernels of these four convolution layers have size $3\times 3$.
They spread evenly in the 13-layer network.
Other layers only have kernels of size $1\times 1$, which can then be viewed as computation inside unit cells.
The rationale behind this is that the messages likely need to be processed before the next round of communication.
The batch normalization layers also spread evenly, with the hope that they can make the training more stable.

After the renormalization process reduces the lattice to a size of $2\times 2$, we apply 4 dense layers (a.k.a fully-connected layers).
Note that the dense layers conveniently break the translational symmetry imposed by the convolution layers.
In the end, we have a neural network with input shape $(L,L,3)$ and output shape $(2)$\footnote{For efficient training, an additional dimension called batch size will be added.}.
The input shape is $(L,L,3)$ because this is the input shape of BP networks.
For $L=64$, the total number of trainable layers in the network is around 60, which is very large compared to early deep neural networks~\cite{Krizhevsky2012imagenet}.
However, most of the computation cost and the trainable parameters are concentrated in the 16 convolution layers with kernel size $3\times 3$.
Combining this and the careful training strategy we describe below, we find that the training can be done very efficiently.

\subsection{Training}
\label{subsec:training}
In general, training neural networks becomes harder when the number of layers increases.
This is often attributed to the instability of gradient backpropagation.
Considering we have a very deep neural network, we should find a way to train parts of the network first.
The training is divided into two stages.
First, we train the belief propagation network to indeed do belief propagation (BP).
This corresponding the blue circle with 0 in \autoref{subfig:complete_network}.
To do this, we implement a BP algorithm and use it to generate training data for the network.
More concretely, we first assign a random error rate $e^{-k}$ to each edge, where $k\in [0.7,7]$ from a uniform distribution.
The choice of the distribution is quite arbitrary.
Then we sample error on each edge according to its error rate and compute the syndrome.
After that, we feed both the error rates and syndrome into our handcrafted BP algorithm, which will output an estimation of the error rates $p_e$ corresponding to the coarse-grained edges.
We can subsequently train the BP network with the same input-output relation.
An important detail is that we transform the error rates $p_e(1)$ in both input and output to $r_e=\log \left(p_e(1)/p_e(0)\right)$.
The reason behind this is described in \aref{appendix:simpler_approach}. 

Next, we load the pre-trained belief propagation network into the decoder network described in the previous subsection.
To ensure $r_e$ stay bounded, we perform a rescale $r_e \rightarrow 7r_e/\max_e |r_e|$ before feed it into next RN block (the choice of 7 here is arbitrary).
We can then train the dense layers and afterward the whole network with input-output pairs (syndrome, logical correction).
These two trainings correspond to the blue circle 1 and 2 in \autoref{subfig:complete_network}, respectively.
The training data is measurable in experiments in these two training.

We train the decoders for different lattice sizes $L$ separately.
Although this makes the concept of threshold pointless, it is still useful to estimate the ``threshold'' so that we can have a rough comparison of the neural decoder with the existing ones.
For this, we train the decoder for different $L$ with the same amount of stochastic gradient steps, which also implies the optimizer sees the same amount of training data for each $L$.
In addition, the training for each $L$ is done under 1 hour (on the year 2016 personal computer with 1 GPU).
We consider this to be a fairly strict policy.
The result is plotted in \autoref{fig:logical_vs_physical}.
We can also forgo this strict policy and spend more time in training the neural decoder for $d=64$ toric code, which gives rise to \autoref{fig:comparison_to_MWPM}.
The training time is still under 2 hours.
More details about training can be found in \aref{appendix:details}, and more discussion about the numerical results can be found in the following section.
